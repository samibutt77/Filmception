{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8eb2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import string\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\Ashfaq Ahmed\\\\Desktop\\\\archive\\\\MovieSummaries\\\\movie_metadata.tsv', delimiter='\\t', header=None)\n",
    "\n",
    "df.columns = [\n",
    "    'movie_id', 'freebase_id', 'title', 'release_date', 'box_office',\n",
    "    'runtime', 'languages', 'countries', 'genres'\n",
    "]\n",
    "\n",
    "def extract_genre_list(genre_str):\n",
    "    try:\n",
    "        genre_dict = ast.literal_eval(genre_str)\n",
    "        return list(genre_dict.values())\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "df['genre_list'] = df['genres'].apply(extract_genre_list)\n",
    "\n",
    "# Step 1: Read movie_id and genre from TSV\n",
    "df_tsv = pd.read_csv(\"C:\\\\Users\\\\Ashfaq Ahmed\\\\Desktop\\\\archive\\\\MovieSummaries\\\\movie_metadata.tsv\", sep=\"\\t\", header=None)\n",
    "df_tsv.columns = ['movie_id', 'freebase_id', 'title', 'release_date', 'box_office',\n",
    "                  'runtime', 'languages', 'countries', 'genres']\n",
    "\n",
    "# Convert genre dict string to a list of genre names\n",
    "def extract_genre_list(genre_str):\n",
    "    try:\n",
    "        genre_dict = ast.literal_eval(genre_str)\n",
    "        return list(genre_dict.values())\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "df_tsv['genre_list'] = df_tsv['genres'].apply(extract_genre_list)\n",
    "\n",
    "# Keep only movie_id and genre_list\n",
    "df_genres = df_tsv[['movie_id', 'genre_list']]\n",
    "\n",
    "# Step 2: Read summaries from TXT file and make a dictionary\n",
    "summary_dict = {}\n",
    "\n",
    "with open(\"C:\\\\Users\\\\Ashfaq Ahmed\\\\Desktop\\\\archive\\\\MovieSummaries\\\\plot_summaries.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\", 1)  # split into movie_id and summary\n",
    "        if len(parts) == 2:\n",
    "            movie_id, summary = parts\n",
    "            summary_dict[int(movie_id)] = summary  # store as int for matching\n",
    "\n",
    "# Step 3: Match summaries to movie IDs\n",
    "df_genres['summary'] = df_genres['movie_id'].apply(lambda x: summary_dict.get(x, \"\"))\n",
    "\n",
    "# Step 4: Drop entries with missing summaries (optional)\n",
    "df_final = df_genres[df_genres['summary'] != \"\"]\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df_final.to_csv(\"final_cleaned_data.csv\", index=False)\n",
    "\n",
    "\n",
    "df2 = pd.read_csv('final_cleaned_data.csv')\n",
    "\n",
    "\n",
    "df2['genre_list'] = df2['genre_list'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "def remove_noise(data):\n",
    "    data = re.sub(r'\\s+', ' ', data)\n",
    "    data = re.sub(r'<.*?>', '', data)\n",
    "    data = re.sub(r'[^a-zA-Z0-9\\s]', '', data)\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'[\\d]', '', data)                      # Remove digits\n",
    "    data = data.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    return data\n",
    "\n",
    "df2['Clean_summaries'] = df2['summary'].apply(remove_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b398ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Initialize Python porter stemmer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lemmatized = [wnl.lemmatize(word, pos=\"v\") for word in tokens]  # pos=\"v\" for verbs\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply it to your cleaned summaries\n",
    "df2['lemmatized_summary'] = df2['Clean_summaries'].apply(tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a69dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df2['lemmatized_summary']\n",
    "y = df2['genre_list']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2['lemmatized_summary'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96363498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_vectorized = cv.fit_transform(x_train)\n",
    "X_test_vectorized = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8550c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_updated = mlb.fit_transform(y_train)\n",
    "y_test_updated = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train_vectorized, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fcd26b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "the film open in edinburgh with a narration by angus the hang ##man tell of how the corpses of those hang by himself be transport to dr robert knox to di ##sse ##ct while his rival dr alexander mon ##...\n",
      "Predicted Genres: ('Crime Fiction', 'Drama')\n",
      "Actual Genres: ('Comedy', 'Thriller')\n",
      "\n",
      "Summary:\n",
      "the story start as one of the robots fly into a scientists secret lair and un ##load ##s a pile of cash into a vault the robot be control completely from the scientists command center and we see that ...\n",
      "Predicted Genres: ('Animation', 'Science Fiction', 'Short Film', 'Superhero movie')\n",
      "Actual Genres: ('Action', 'Adventure', 'Animation', 'Family Film', 'Fantasy', 'Science Fiction', 'Short Film')\n",
      "\n",
      "Summary:\n",
      "the film main character be mo char ##a sc ##ud murphy and ce ##re ##ber ##al paul ##sy all from west belfast they have an interest in dog race which the narrator mo char ##a inform us be very importan...\n",
      "Predicted Genres: ('Adventure', 'Animal Picture', \"Children's\", 'Comedy film', 'Family Film', 'Family-Oriented Adventure')\n",
      "Actual Genres: ('Buddy film', 'Comedy')\n",
      "\n",
      "Summary:\n",
      "ol ##o jed ##lina be bear a lil ##ip ##ut and serve drawer ##land government he be permit to visit kings ##aj ##z however after get few drop of potion that make him humans ##ized he decide to stay in ...\n",
      "Predicted Genres: ('Action', 'Adventure', 'Animation', 'Drama')\n",
      "Actual Genres: ('Comedy', 'Drama', 'Fantasy', 'World cinema')\n",
      "\n",
      "Summary:\n",
      "harry be a mis ##ant ##hr ##op ##ic french literature professor who be just take his life one day at a time get by with just enough to survive his ex ##wife amanda be now marry to harry ##s best frien...\n",
      "Predicted Genres: ('Comedy', 'Romance Film', 'Romantic comedy', 'Romantic drama')\n",
      "Actual Genres: ('Short Film',)\n",
      "Accuracy Score: 0.054371002132196165\n",
      "Hamming Loss: 0.010103019852323473\n",
      "F1 Score (micro): 0.3859867246876692\n",
      "F1 Score (macro): 0.09257696219098217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashfaq Ahmed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss\n",
    "\n",
    "# Vectorize test data\n",
    "X_test_vectorized = cv.transform(x_test)  # Note: use transform, NOT fit_transform\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test_vectorized)\n",
    "\n",
    "# Convert predicted and actual genres back to labels\n",
    "predicted_genres = mlb.inverse_transform(y_pred)\n",
    "actual_genres = mlb.inverse_transform(y_test_updated)\n",
    "\n",
    "# Show first 5 predictions\n",
    "for i in range(5):\n",
    "    print(f\"\\nSummary:\\n{x_test.iloc[i][:200]}...\")\n",
    "    print(\"Predicted Genres:\", predicted_genres[i])\n",
    "    print(\"Actual Genres:\", actual_genres[i])\n",
    "\n",
    "# Evaluation Metrics\n",
    "acc = accuracy_score(y_test_updated, y_pred)\n",
    "print(\"Accuracy Score:\", acc)\n",
    "print(\"Hamming Loss:\", hamming_loss(y_test_updated, y_pred))\n",
    "print(\"F1 Score (micro):\", f1_score(y_test_updated, y_pred, average='micro'))\n",
    "print(\"F1 Score (macro):\", f1_score(y_test_updated, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ba8516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Genres: ('Crime Fiction', 'Drama')\n"
     ]
    }
   ],
   "source": [
    "summary = input(\"Enter a movie summary:\\n\")\n",
    "\n",
    "# Step 1: Preprocess if needed\n",
    "cleaned_summary = remove_noise(summary)  # Apply same preprocessing used in training\n",
    "\n",
    "Lemmatized_summary = tokenize_and_lemmatize(cleaned_summary)\n",
    "\n",
    "# Step 2: Vectorize using trained CountVectorizer\n",
    "summary_vectorized = cv.transform([Lemmatized_summary])  # Note: list input\n",
    "\n",
    "# Step 3: Predict using trained classifier\n",
    "prediction = clf.predict(summary_vectorized)\n",
    "\n",
    "# Step 4: Inverse transform to get genre labels\n",
    "predicted_genres = mlb.inverse_transform(prediction)\n",
    "\n",
    "# Step 5: Output result\n",
    "print(\"Predicted Genres:\", predicted_genres[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import time\n",
    "import sentencepiece\n",
    "\n",
    "# Define a reusable translation function\n",
    "def load_model_and_tokenizer(src_lang, tgt_lang):\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def translate_text(text, model, tokenizer):\n",
    "    batch = tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    gen = model.generate(**batch)\n",
    "    return tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "# Load your dataset\n",
    "df3 = pd.read_csv(\"C:\\\\Users\\\\Ashfaq Ahmed\\\\Desktop\\\\final_cleaned_data.csv\")  # Replace with your filename\n",
    "\n",
    "# Language pairs you want to translate to\n",
    "lang_pairs = {\n",
    "    \"ar\": (\"en\", \"ar\"),\n",
    "    \"ur\": (\"en\", \"ur\"),\n",
    "}\n",
    "\n",
    "#removed_noise_summary = df3['summary'].apply(remove_noise)\n",
    "#lemmatized_summary_for_translation = removed_noise_summary.apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Translate each cleaned summary\n",
    "for lang_code, (src, tgt) in lang_pairs.items():\n",
    "    print(f\"Loading model for {src} → {tgt}\")\n",
    "    model, tokenizer = load_model_and_tokenizer(src, tgt)\n",
    "\n",
    "    translations = []\n",
    "    for i, text in enumerate(df2['lemmatized_summary'][:5]):\n",
    "        try:\n",
    "            translated = translate_text(text, model, tokenizer)\n",
    "            translations.append(translated)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at row {i}: {e}\")\n",
    "            translations.append(\"\")\n",
    "        time.sleep(0.1)  # Optional: prevent memory overload\n",
    "\n",
    "    df3.loc[:4, f'summary_{lang_code}'] = translations\n",
    "\n",
    "# Save the translated data\n",
    "df3.to_csv(\"translated_movie_data.csv\", index=False)\n",
    "print(\"Translation complete. Saved to translated_movie_data.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
